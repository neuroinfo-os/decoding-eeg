{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# Problem 1\n",
    "In this problem, we will generate bivariate data from two normal distributions and classify the data using logistic regression. The function `modelFitVal` will fit and cross-validate the model. We will visualize the underlying distributions and the resulting decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "%pylab inline\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Sampling and Analyzing\n",
    "We begin by specifying the parameters for generating the data: sample size, means, and covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set sample size, means, and covariance matrix\n",
    "nSamples = 500\n",
    "mu_1 = [1, 3]\n",
    "mu_2 = [3, 0]\n",
    "sigma = [[ 2.0,  1.5],\n",
    "         [ 1.5,  2.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate the data using these parameters to sample from a multidimensional normal distribution, concatenate the two\n",
    "sample populations, and create a vector of labels (ones for one population, zeros for the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data from both distributions\n",
    "X1 = ...\n",
    "X2 = ...\n",
    "X = vstack([X1, X2])\n",
    "\n",
    "# create labels\n",
    "L1 = ...\n",
    "L2 = ...\n",
    "L = concatenate([L1, L2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n$ is the number of samples, $X$ will be a $2n\\times 2$ matrix and $L$ will be of length $2n$.\n",
    "Next, we want to see what the two underlying distributions look like along the two dimensions $x_1$\n",
    "and $x_2$ (not to be confused with the two sample populations $X_1$ and $X_2$). Create a grid of $100\\times100$\n",
    "equidistant points within the bounds of the sample populations, using xVector, and compute the two\n",
    "PDFs for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute representative densities of underlying distributions\n",
    "xVector = linspace(X.min(), X.max(), 100)\n",
    "...\n",
    "pDist = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to use the `meshgrid` function and reshape the resulting matrices to vectors. If you have trouble with the suggested one-liners, use for-loops instead. The result `pDist` should be a 100$\\times$100 matrix containing the sum of both PDFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a logistic regression on the sample data, using the implementation that is included in the sklearn package. [[link]](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) We use it plain, without modifying any paramters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the decision boundary of the fitted model, we solve the iso-error problem, i.e. we compute the line on which $P(C_1\\ |\\ x) = P(C_2\\ |\\ x) = 0.5$. This means solving\n",
    "$$0.5 = \\sigma (w^Tx+w_0)$$\n",
    "with $\\sigma$ being the logistic function; $w=(w_1,w_2)$; $w_0$ is the intercept and $x=(x_1,x_2)$. We have to rewrite the equation for $x_2$ and set $x_1$ to `xVector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the decision boundary of the fitted model\n",
    "w0 = model.intercept_[0]\n",
    "w1, w2 = model.coef_[0]\n",
    "x2db = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we validate the fitted model (<span style=\"color: #666666\">*Hint: use model.predict_proba*</span>) on the same 100$\\times$100 data points as before to get the posterior densities predicted by the model. (We only compute $P(C_1\\ |\\ x)$, since $P(C_2\\ |\\ x)=1-P(C_1\\ |\\ x)$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute representative class posteriors\n",
    "posterior_C1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Classification\n",
    "The function `modelFitVal` should take a data matrix (samples $\\times$ dimensions), a correpsonding label vector, and the number of partitions, then iteratively divide the data and labels into training sets and test sets, fit and validate the logistic regression, and return the mean percentage of correctly predicted labels.\n",
    "\n",
    "**a)** First, we create a random permutation of the sample indices, which we will use when composing the partitions.\n",
    "\n",
    "```python\n",
    "nSamples = size(X, 1);\n",
    "randIdx = ...\n",
    "```\n",
    "\n",
    "This way we avoid order effects on model performance but still create disjoint partitions. With our random data, order doesn't matter, but will in the later problems.\n",
    "\n",
    "**b)** In the subsequent loop we divide the data and labels into test and training sets. From `rand_idx` we select $\\frac{\\texttt{nSamples}}{\\texttt{k}}$ indices for testing and the remaining $\\frac{(\\texttt{k}-1)\\ \\cdot\\ \\texttt{nSamples}}{\\texttt{k}}$ indices for training.\n",
    "\n",
    "Note that `nSamples` need not be a multiple of `k`. Make sure that the size of test sets (same for training sets) does not differ by more than 1 across partitions. Make also sure that all samples are used for testing exactly once.\n",
    "\n",
    "**c)** Now we fit the model as before and compare the validation output, i.e. the predicted labels for the test data, to the actual labels. The percentage of correct predictions is then accumulated over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelFitVal(X, L, kPart):\n",
    "    \"\"\"\n",
    "    modelFitVal takes multivariate data (one feature per column) and\n",
    "    associated labels and returns the cross-validated performance of a linear\n",
    "    model.\n",
    "\n",
    "    \"\"\"\n",
    "    # a)\n",
    "    # set up shuffled data partitions\n",
    "    nSamples = len(X)\n",
    "    rand_idx = ...\n",
    "\n",
    "    pCorrect = 0\n",
    "\n",
    "    for iPart in range(kPart):\n",
    "\n",
    "        # b)\n",
    "        # separate training and test data\n",
    "        test_idx = ...\n",
    "        train_idx = ...\n",
    "        \n",
    "        xTest = ...\n",
    "        lTest = ...\n",
    "        xTrain = ...\n",
    "        lTrain = ...\n",
    "        \n",
    "        # c)\n",
    "        # fit model to training data using logistic regression & make predictions\n",
    "        ...\n",
    "        predictions = ...\n",
    "\n",
    "        # compare model's classification to actual labels\n",
    "        pCorrect += mean(predictions == lTest) / kPart\n",
    "\n",
    "    return pCorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your model do get the valiadation error of a ten fold crossvalidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate the same model to estimate generalization\n",
    "pCorrect = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Plotting and Exploring\n",
    "Your code should print the model's average classification performance and produce the two figures below, showing the sample data with their underlying distributions and the model's decision boundary, and the class posterior probabilities, respectively.\n",
    "\n",
    "[Scatter Data + Decision boundary](../../matlab/problem1/p1fig1.png),\n",
    "[C1 posterior](../../matlab/problem1/p1fig2.png)\n",
    "\n",
    "Note that the plots will vary a bi../../matlab/problem1/p1fig1.pngt at between runs because of the randomness in sampling and cross-validation. Classification performance should be at or near 100%. You may want to try different distribution parameters and sample sizes and see how performance and plots change. Try to understand the warning messages you might get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data, distributions, and decision boundary\n",
    "figure(figsize=[17,8])\n",
    "subplot(121)\n",
    "imshow(rot90(pDist.T), extent=(X.min(), X.max())*2, cmap='gray')\n",
    "scatter(*X1.T, color='red', marker='x')\n",
    "scatter(*X2.T, color='blue', marker='x')\n",
    "plot(xVector, x2db, color='green')\n",
    "ylim((X.min(), X.max()))\n",
    "xlim((X.min(), X.max()))\n",
    "title('Data and Densities')\n",
    "\n",
    "# plot posterior C1\n",
    "subplot(122)\n",
    "imshow(rot90(posterior_C1), extent=(X.min(), X.max())*2, cmap='jet')\n",
    "ylim(X.min(), X.max()); xlim(X.min(), X.max())\n",
    "title('Posterior p(C1|X)')\n",
    "\n",
    "# print performance\n",
    "print('\\nPerformance: %s percent\\n\\n' % round(100*pCorrect))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
